{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch Bert Model for High Performance in ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll be introduced to how to load a Bert model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "It need a python environment with [PyTorch](https://pytorch.org/) and [OnnxRuntime](https://microsoft.github.io/onnxruntime/) installed before running this notebook. \n",
    "\n",
    "First, we install [AnaConda](https://www.anaconda.com/distribution/) in a target machine and open an AnaConda prompt window when it is done. Then you can choose a setup based on your target device (CPU or GPU), and run the commands to create a conda environment.\n",
    "\n",
    "#### CPU Environment Setup\n",
    "If your machines does not have GPU or want to test CPU inference. You can create a conda environment like the following:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.6\n",
    "conda activate cpu_env\n",
    "conda install pytorch torchvision cpuonly -c pytorch\n",
    "pip install onnxruntime\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue.\n",
    "\n",
    "Another option is to use pip to install package to your existing jupyter notebook environment:\n",
    "```console\n",
    "pip install --upgrade torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "pip install onnxruntime==1.1.2\n",
    "```\n",
    "\n",
    "#### GPU Environment Setup\n",
    "\n",
    "This requires your machine to have a GPU.\n",
    "\n",
    "```console\n",
    "conda create -n gpu_env python=3.6\n",
    "conda activate gpu_env\n",
    "conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "pip install onnxruntime-gpu\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "onnxruntime-gpu v1.1.2 requires installing [CUDA](https://developer.nvidia.com/cuda-downloads) 10.0 and [cuDNN](https://developer.nvidia.com/cudnn) 7.6, and add their bin directories to PATH environment variable (You need update the path in section 4 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install some extra packages used in this notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers==2.5.1\n",
    "!{sys.executable} -m pip install wget\n",
    "!{sys.executable} -m pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Bert model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the data files and store them in the specified location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./pytorch_squad\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify some model config variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine tuned large model, the model name is \"bert-large-uncased-whole-word-masking-finetuned-squad\". Here we use bert-base for demo.\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to load model from pretrained. This step could take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "\n",
    "from transformers import (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "# load some examples\n",
    "from transformers.data.processors.squad import SquadV1Processor\n",
    "\n",
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:3], # convert only 3 examples for demo\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export the loaded model ##\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./pytorch_onnx\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "output_model_path = os.path.join(output_dir, 'bert-base-cased-squad.onnx')\n",
    "\n",
    "# Get the first batch of data to run the model and export it to ONNX\n",
    "batch = dataset[0]\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "inputs = {\n",
    "    'input_ids':      batch[0].to(device).reshape(1, 128),                         # using batch size = 1 here. Adjust as needed.\n",
    "    'attention_mask': batch[1].to(device).reshape(1, 128),\n",
    "    'token_type_ids': batch[2].to(device).reshape(1, 128)\n",
    "}\n",
    "\n",
    "if not os.path.exists(output_model_path):\n",
    "    with torch.no_grad():\n",
    "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "        torch.onnx.export(model,                                            # model being run\n",
    "                          (inputs['input_ids'],                             # model input (or a tuple for multiple inputs)\n",
    "                           inputs['attention_mask'], \n",
    "                           inputs['token_type_ids']), \n",
    "                          output_model_path,                                # where to save the model (can be a file or file-like object)\n",
    "                          opset_version=11,                                 # the ONNX version to export the model to\n",
    "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                          input_names=['input_ids',                         # the model's input names\n",
    "                                       'input_mask', \n",
    "                                       'segment_ids'],\n",
    "                          output_names=['start', 'end'],                    # the model's output names\n",
    "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                        'input_mask' : symbolic_names,\n",
    "                                        'segment_ids' : symbolic_names,\n",
    "                                        'start' : {0: 'batch_size'},\n",
    "                                        'end' : {0: 'batch_size'}})\n",
    "        print(\"Model exported at \", output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Inference ##\n",
    "Use PyTorch to evaluate an example input for comparison purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Warm up with one run.\n",
    "model(**inputs)\n",
    "\n",
    "# Measure the latency.\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    outputs = model(**inputs)\n",
    "    end = time.time()\n",
    "    print(\"PyTorch {} Inference time = {} ms\".format(device.type, format((end - start) * 1000, '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference the Exported Model with ONNX Runtime ##\n",
    "\n",
    "To use onnxruntime-gpu, it is required to install CUDA 10.0 and CUDNN 7.6, and add their bin directories to PATH environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    # Add path for CUDA 10.0 and CUDNN 7.6, which are required by onnxruntime-gpu\n",
    "    cuda_dir = 'E:/NVidia/CUDA/v10.0/bin'\n",
    "    cudnn_dir = 'E:/NVidia/CUDA/v10.0/bin'\n",
    "    if not (os.path.exist(cuda_dir) and os.path.exist(cudnn_dir)):\n",
    "        raise ValueError(\"Please specify correct path for CUDA 10.0 and CUDNN 7.6. Otherwise onnxruntime-gpu cannot be imported.\")\n",
    "    else:\n",
    "        if cuda_dir == cudnn_dir:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + os.environ[\"PATH\"]\n",
    "        else:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + cudnn_dir + ';' + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference the model with ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "device_name = 'cuda' if 'CUDAExecutionProvider' in onnxruntime.get_available_providers() else 'cpu'\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "#sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
    "\n",
    "# The following settings enables OpenMP, which is required to get best performance for CPU inference of Bert models.\n",
    "sess_options.intra_op_num_threads=1\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "session = onnxruntime.InferenceSession(output_model_path, sess_options)\n",
    "\n",
    "# Use contiguous array as input could improve performance.\n",
    "ort_inputs = {'input_ids': numpy.ascontiguousarray(inputs['input_ids'].cpu().numpy()),\n",
    "              'input_mask': numpy.ascontiguousarray(inputs['attention_mask'].cpu().numpy()),\n",
    "              'segment_ids': numpy.ascontiguousarray(inputs['token_type_ids'].cpu().numpy())\n",
    "}\n",
    "\n",
    "# Warm up with one run.\n",
    "session.run(None, ort_inputs)\n",
    "\n",
    "# Measure the latency.\n",
    "start = time.time()\n",
    "results = session.run(None, ort_inputs)\n",
    "end = time.time()\n",
    "print(\"ONNX Runtime {} inference time: {} ms\".format(device_name, format((end - start) * 1000, '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"***** Verifying correctness *****\")\n",
    "for i in range(2):\n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(results[i], outputs[i].cpu(), rtol=1e-05, atol=1e-04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test Tools\n",
    "For more accurate latency number, it is recommended to download the [Performance Test and Model Verification Tools](https://github.com/microsoft/onnxruntime/tree/tlwu/bert_test_tools/onnxruntime/python/tools/bert) and run them on the exported models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "url_prfix = \"https://raw.githubusercontent.com/microsoft/onnxruntime/tlwu/bert_test_tools/onnxruntime/python/tools/bert/\"\n",
    "script_files = ['bert_perf_test.py', 'compare_bert_results.py', 'BertOnnxModel.py', 'BertOnnxModelKeras.py', 'BertOnnxModelTF.py', 'OnnxModel.py', 'bert_model_optimization.py']\n",
    "\n",
    "script_dir = './bert_scripts'\n",
    "if not os.path.exists(script_dir):\n",
    "    os.makedirs(script_dir)\n",
    "\n",
    "for filename in script_files:\n",
    "    target_file = os.path.join(script_dir, filename)\n",
    "    if not os.path.exists(target_file):\n",
    "        wget.download(url_prfix + filename, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install onnx pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will create 10 samples (of batch_size 1 and sequence_length 128) and run each for 10 times, then get the average latency number. You can increase number of samples to get more stable result, while the test will take longer time to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./bert_scripts/bert_perf_test.py --model ./pytorch_onnx/bert-base-cased-squad.onnx --batch_size 1 --sequence_length 128 --samples 10 --test_times 10 --inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./bert_scripts/compare_bert_results.py --baseline_model ./pytorch_onnx/bert-base-cased-squad.onnx --batch_size 1 --sequence_length 128 --samples 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

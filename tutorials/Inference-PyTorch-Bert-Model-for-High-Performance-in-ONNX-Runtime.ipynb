{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch Bert Model for High Performance in ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll be introduced to how to load a Bert model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "It need a python environment with [PyTorch](https://pytorch.org/) and [OnnxRuntime](https://microsoft.github.io/onnxruntime/) installed before running this notebook. \n",
    "\n",
    "First, we install [AnaConda](https://www.anaconda.com/distribution/) in a target machine and open an AnaConda prompt window when it is done. Then you can choose a setup based on your target device (CPU or GPU), and run the commands to create a conda environment.\n",
    "\n",
    "#### CPU Environment Setup\n",
    "If your machines does not have GPU or want to test CPU inference. You can create a conda environment like the following:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.6\n",
    "conda activate cpu_env\n",
    "conda install pytorch torchvision cpuonly -c pytorch\n",
    "pip install onnxruntime\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue.\n",
    "\n",
    "Another option is to use pip to install package to your existing jupyter notebook environment:\n",
    "```console\n",
    "pip install --upgrade torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "pip install onnxruntime==1.1.2\n",
    "```\n",
    "\n",
    "#### GPU Environment Setup\n",
    "\n",
    "This requires your machine to have a GPU.\n",
    "\n",
    "```console\n",
    "conda create -n gpu_env python=3.6\n",
    "conda activate gpu_env\n",
    "conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "pip install onnxruntime-gpu\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "onnxruntime-gpu v1.1.2 requires installing [CUDA](https://developer.nvidia.com/cuda-downloads) 10.0 and [cuDNN](https://developer.nvidia.com/cudnn) 7.6, and add their bin directories to PATH environment variable (You need update the path in section 4 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.5.1\n",
      "  Using cached transformers-2.5.1-py3-none-any.whl (499 kB)\n",
      "Processing c:\\users\\tianl\\appdata\\local\\pip\\cache\\wheels\\6d\\ec\\1a\\21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\\sacremoses-0.0.38-cp36-none-any.whl\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.85-cp36-cp36m-win_amd64.whl (1.2 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.12.11-py2.py3-none-any.whl (128 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-win_amd64.whl (272 kB)\n",
      "Collecting tokenizers==0.5.2\n",
      "  Using cached tokenizers-0.5.2-cp36-cp36m-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (1.18.1)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from sacremoses->transformers==2.5.1) (1.14.0)\n",
      "Collecting click\n",
      "  Using cached Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting botocore<1.16.0,>=1.15.11\n",
      "  Downloading botocore-1.15.11-py2.py3-none-any.whl (5.9 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from requests->transformers==2.5.1) (2019.11.28)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from botocore<1.16.0,>=1.15.11->boto3->transformers==2.5.1) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Installing collected packages: joblib, tqdm, regex, click, sacremoses, sentencepiece, filelock, docutils, urllib3, jmespath, botocore, s3transfer, boto3, idna, chardet, requests, tokenizers, transformers\n",
      "Successfully installed boto3-1.12.11 botocore-1.15.11 chardet-3.0.4 click-7.0 docutils-0.15.2 filelock-3.0.12 idna-2.9 jmespath-0.9.5 joblib-0.14.1 regex-2020.2.20 requests-2.23.0 s3transfer-0.3.3 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 tqdm-4.43.0 transformers-2.5.1 urllib3-1.25.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'D:\\Anaconda3\\envs\\torch14_gpu\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script sacremoses.exe is installed in 'D:\\Anaconda3\\envs\\torch14_gpu\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script chardetect.exe is installed in 'D:\\Anaconda3\\envs\\torch14_gpu\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\tianl\\appdata\\local\\pip\\cache\\wheels\\40\\15\\30\\7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\\wget-3.2-cp36-none-any.whl\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Collecting psutil\n",
      "  Using cached psutil-5.7.0-cp36-cp36m-win_amd64.whl (235 kB)\n",
      "Installing collected packages: psutil\n",
      "Successfully installed psutil-5.7.0\n"
     ]
    }
   ],
   "source": [
    "# install some extra packages used in this notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers==2.5.1\n",
    "!{sys.executable} -m pip install wget\n",
    "!{sys.executable} -m pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Bert model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the data files and store them in the specified location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./squad\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify some model config variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine tuned large model, the model name is \"bert-large-uncased-whole-word-masking-finetuned-squad\". Here we use bert-base for demo.\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to load model from pretrained. This step could take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:03<00:00, 13.88it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 125.49it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 2926.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "\n",
    "from transformers import (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "# load some examples\n",
    "from transformers.data.processors.squad import SquadV1Processor\n",
    "\n",
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:3], # convert only 3 examples for demo\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export the loaded model ##\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  ./onnx\\bert-base-cased-squad.onnx\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./onnx\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "output_model_path = os.path.join(output_dir, 'bert-base-cased-squad.onnx')\n",
    "\n",
    "# Get the first batch of data to run the model and export it to ONNX\n",
    "batch = dataset[0]\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "inputs = {\n",
    "    'input_ids':      batch[0].to(device).reshape(1, 128),                         # using batch size = 1 here. Adjust as needed.\n",
    "    'attention_mask': batch[1].to(device).reshape(1, 128),\n",
    "    'token_type_ids': batch[2].to(device).reshape(1, 128)\n",
    "}\n",
    "\n",
    "if not os.path.exists(output_model_path):\n",
    "    with torch.no_grad():\n",
    "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "        torch.onnx.export(model,                                            # model being run\n",
    "                          (inputs['input_ids'],                             # model input (or a tuple for multiple inputs)\n",
    "                           inputs['attention_mask'], \n",
    "                           inputs['token_type_ids']), \n",
    "                          output_model_path,                                # where to save the model (can be a file or file-like object)\n",
    "                          opset_version=11,                                 # the ONNX version to export the model to\n",
    "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                          input_names=['input_ids',                         # the model's input names\n",
    "                                       'input_mask', \n",
    "                                       'segment_ids'],\n",
    "                          output_names=['start', 'end'],                    # the model's output names\n",
    "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                        'input_mask' : symbolic_names,\n",
    "                                        'segment_ids' : symbolic_names,\n",
    "                                        'start' : {0: 'batch_size'},\n",
    "                                        'end' : {0: 'batch_size'}})\n",
    "        print(\"Model exported at \", output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Inference ##\n",
    "Use PyTorch to evaluate an example input for comparison purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cuda Inference time = 30.92 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Warm up with one run.\n",
    "model(**inputs)\n",
    "\n",
    "# Measure the latency.\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    outputs = model(**inputs)\n",
    "    end = time.time()\n",
    "    print(\"PyTorch {} Inference time = {} ms\".format(device.type, format((end - start) * 1000, '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference the Exported Model with ONNX Runtime ##\n",
    "\n",
    "To use onnxruntime-gpu, it is required to install CUDA 10.0 and CUDNN 7.6, and add their bin directories to PATH environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    # Add path for CUDA 10.0 and CUDNN 7.6, which are required by onnxruntime-gpu\n",
    "    cuda_dir = 'D:/NVidia/CUDA/v10.0/bin'\n",
    "    cudnn_dir = 'D:/NVidia/CUDA/v10.0/bin'\n",
    "    if not (os.path.exists(cuda_dir) and os.path.exists(cudnn_dir)):\n",
    "        raise ValueError(\"Please specify correct path for CUDA 10.0 and CUDNN 7.6. Otherwise onnxruntime-gpu cannot be imported.\")\n",
    "    else:\n",
    "        if cuda_dir == cudnn_dir:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + os.environ[\"PATH\"]\n",
    "        else:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + cudnn_dir + ';' + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference the model with ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime cuda inference time: 9.97 ms\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "device_name = 'cuda' if 'CUDAExecutionProvider' in onnxruntime.get_available_providers() else 'cpu'\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "#sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
    "\n",
    "# The following settings enables OpenMP, which is required to get best performance for CPU inference of Bert models.\n",
    "sess_options.intra_op_num_threads=1\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "session = onnxruntime.InferenceSession(output_model_path, sess_options)\n",
    "\n",
    "# Use contiguous array as input could improve performance.\n",
    "ort_inputs = {'input_ids': numpy.ascontiguousarray(inputs['input_ids'].cpu().numpy()),\n",
    "              'input_mask': numpy.ascontiguousarray(inputs['attention_mask'].cpu().numpy()),\n",
    "              'segment_ids': numpy.ascontiguousarray(inputs['token_type_ids'].cpu().numpy())\n",
    "}\n",
    "\n",
    "# Warm up with one run.\n",
    "session.run(None, ort_inputs)\n",
    "\n",
    "# Measure the latency.\n",
    "start = time.time()\n",
    "results = session.run(None, ort_inputs)\n",
    "end = time.time()\n",
    "print(\"ONNX Runtime {} inference time: {} ms\".format(device_name, format((end - start) * 1000, '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Verifying correctness *****\n",
      "PyTorch and ONNX Runtime output 0 are close: True\n",
      "PyTorch and ONNX Runtime output 1 are close: True\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Verifying correctness *****\")\n",
    "for i in range(2):\n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(results[i], outputs[i].cpu(), rtol=1e-05, atol=1e-04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test Tools\n",
    "For more accurate latency number, it is recommended to download the [Performance Test and Model Verification Tools](https://github.com/microsoft/onnxruntime/tree/tlwu/bert_test_tools/onnxruntime/python/tools/bert) and run them on the exported models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [................................................................................] 7605 / 7605"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "url_prfix = \"https://raw.githubusercontent.com/microsoft/onnxruntime/tlwu/bert_test_tools/onnxruntime/python/tools/bert/\"\n",
    "script_files = ['bert_perf_test.py', 'compare_bert_results.py', 'BertOnnxModel.py', 'BertOnnxModelKeras.py', 'BertOnnxModelTF.py', 'OnnxModel.py', 'bert_model_optimization.py']\n",
    "\n",
    "script_dir = './bert_scripts'\n",
    "if not os.path.exists(script_dir):\n",
    "    os.makedirs(script_dir)\n",
    "\n",
    "for filename in script_files:\n",
    "    target_file = os.path.join(script_dir, filename)\n",
    "    if not os.path.exists(target_file):\n",
    "        wget.download(url_prfix + filename, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.6.0-cp36-cp36m-win_amd64.whl (4.5 MB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Collecting typing-extensions>=3.6.2.1\n",
      "  Downloading typing_extensions-3.7.4.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from onnx) (1.14.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from onnx) (1.18.1)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.11.3-cp36-cp36m-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from protobuf->onnx) (45.2.0.post20200210)\n",
      "Installing collected packages: typing-extensions, protobuf, onnx, pytz\n",
      "Successfully installed onnx-1.6.0 protobuf-3.11.3 pytz-2019.3 typing-extensions-3.7.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts backend-test-tools.exe, check-model.exe and check-node.exe are installed in 'D:\\Anaconda3\\envs\\torch14_gpu\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install onnx pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will create 10 samples (of batch_size 1 and sequence_length 128) and run each for 10 times, then get the average latency number. You can increase number of samples to get more stable result, while the test will take longer time to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating test data...\n",
      "Running test: model=bert-base-cased-squad.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,OMP_NUM_THREADS=,OMP_WAIT_POLICY=,batch_size=1,sequence_length=128,test_cases=10,test_times=10,contiguous=False,use_gpu=True\n",
      "Average latency is 8.72 ms\n",
      "Extra latency for converting inputs to contiguous: 0.00 ms\n",
      "Running test: model=bert-base-cased-squad.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,OMP_NUM_THREADS=,OMP_WAIT_POLICY=,batch_size=1,sequence_length=128,test_cases=10,test_times=10,contiguous=True,use_gpu=True\n",
      "Average latency is 8.87 ms\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1_S128_20200301-225917.txt\n"
     ]
    }
   ],
   "source": [
    "%run ./bert_scripts/bert_perf_test.py --model ./onnx/bert-base-cased-squad.onnx --batch_size 1 --sequence_length 128 --samples 10 --test_times 10 --inclusive --use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline average latency: 31.352509999999256 ms\n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_0 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_1 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_2 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_3 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_4 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_5 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_6 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_7 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_8 \n",
      "Successfully created the directory onnx\\batch_1_seq_128\\test_data_set_9 \n",
      "treatment average latency: 12.36652000001186 ms\n",
      "0 out of 10 results are not close (rtol=0.001, atol=0.0001).\n",
      "maximum absolute difference=1.0132789611816406e-06 in test case 6\n",
      "maximum relative difference=0.0009318445809185505 in test case 5\n"
     ]
    }
   ],
   "source": [
    "%run ./bert_scripts/compare_bert_results.py --baseline_model ./onnx/bert-base-cased-squad.onnx --batch_size 1 --sequence_length 128 --samples 10 --use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
